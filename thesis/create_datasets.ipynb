{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd82f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pytz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae406122",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_folder = \"../datasets/original\"\n",
    "exp_folder = \"../datasets/experiment\"\n",
    "\n",
    "orig_dataset1 = os.path.join(orig_folder, \"1\")\n",
    "orig_dataset2 = os.path.join(orig_folder, \"2/Training-validation\")\n",
    "orig_dataset3 = os.path.join(orig_folder, \"3\")\n",
    "\n",
    "orig_dataset2_modified = os.path.join(orig_folder, \"2/2-modified/Training-validation\")\n",
    "\n",
    "exp_dataset1 = os.path.join(exp_folder, \"1\")\n",
    "exp_dataset2 = os.path.join(exp_folder, \"2\")\n",
    "exp_dataset3 = os.path.join(exp_folder, \"3\")\n",
    "exp_dataset4 = os.path.join(exp_folder, \"4\")\n",
    "\n",
    "CLASS_NAMES = ['normal', 'aom', 'ome', 'csom', 'myringosclerosis', 'earwax', 'tube']\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "randomiser = np.random.RandomState(RANDOM_STATE)\n",
    "\n",
    "def timer(func):\n",
    "    # This function shows the execution time of \n",
    "    # the function object passed\n",
    "    def wrap_func(*args, **kwargs):\n",
    "        t1 = time()\n",
    "        result = func(*args, **kwargs)\n",
    "        t2 = time()\n",
    "        print(f'Function {func.__name__!r} executed in {(t2-t1):.4f}s')\n",
    "        return result\n",
    "    return wrap_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21cf331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Used to load the original datasets\n",
    "\"\"\"\n",
    "@timer\n",
    "def load_dataset(path, class_names):\n",
    "    path = path\n",
    "    full_data = []\n",
    "    \n",
    "    class_dict = {}\n",
    "    for i, name in enumerate(class_names):\n",
    "        class_dict[name] = i\n",
    "    \n",
    "    for d in class_names:\n",
    "        dirpath = os.path.join(path, d)\n",
    "        if not os.path.exists(dirpath): continue\n",
    "        image_files = [f for f in os.listdir(dirpath) if f.endswith(('.jpg', '.png', 'jpeg'))]\n",
    "        label = d\n",
    "        for img in image_files:\n",
    "            image = Image.open(os.path.join(dirpath, img))\n",
    "            \n",
    "            image = tf.cast(image, tf.float32)/255.0\n",
    "            \n",
    "            data = np.array([image, class_dict[label]], dtype=object)\n",
    "            full_data.append(data)\n",
    "    \n",
    "    randomiser.shuffle(np.array(full_data))\n",
    "    return full_data\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ef584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Splits the complete dataset and returns the train, validation and test sets\n",
    "e.g. train = 0.7, validation = 0.1 splits the train:val:test into 70:10:20\n",
    "\"\"\"\n",
    "@timer\n",
    "def split_data(full_data, train, validation):\n",
    "    X_full = np.array([x[0] for x in full_data], dtype=object)\n",
    "    y_full = np.array([y[1] for y in full_data])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_full, y_full, train_size=train, stratify=y_full, random_state=RANDOM_STATE)\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    \n",
    "\n",
    "    validation = validation / (1.0 - train)\n",
    "    \n",
    "    if validation >= 1:\n",
    "        X_val = X_test\n",
    "        y_val = y_test\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        \n",
    "    elif validation > 0:\n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "                X_test, y_test, train_size=validation, stratify=y_test, random_state=RANDOM_STATE)\n",
    "    \n",
    "    return [(X_train, y_train), (X_val, y_val), (X_test, y_test)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a579567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Peforms data augmentation by a specific factor for a particular class\n",
    "\"\"\"\n",
    "@timer\n",
    "def data_augmentation(num, X_train, y_train, da_dict={}):\n",
    "    training_set = list(zip(X_train, y_train))\n",
    "        \n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=180,\n",
    "        horizontal_flip = True,\n",
    "        vertical_flip = True,\n",
    "        fill_mode='constant',\n",
    "        cval=0.0,\n",
    "        brightness_range=[0.7,1.0],\n",
    "        zoom_range=[0.8,1.2],\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "    )\n",
    "\n",
    "    for i in range(len(training_set)):\n",
    "        img = training_set[i][0]\n",
    "        label = training_set[i][1]\n",
    "\n",
    "        img = np.expand_dims(img, 0)\n",
    "        aug_iter = datagen.flow(img)\n",
    "\n",
    "        count = num // np.bincount(y_train)[label]\n",
    "        if len(da_dict.keys()) > 0:\n",
    "            count = da_dict[class_names[label]]\n",
    "\n",
    "        aug_images = [next(aug_iter)[0] for _ in range(count)]\n",
    "        for ai in aug_images:\n",
    "            data = np.array([ai , label], dtype=object)\n",
    "            training_set.append(data)\n",
    "\n",
    "    randomiser.shuffle(training_set)\n",
    "    X_train = np.array([x[0] for x in training_set], dtype=object)\n",
    "    y_train = np.array([y[1] for y in training_set])\n",
    "    return (X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f08b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Saves the images by naming them with a unique path\n",
    "\"\"\"\n",
    "@timer\n",
    "def save_images(path_to_save, X, y, class_names):\n",
    "    def uniquify(path):\n",
    "        filename, extension = os.path.splitext(path)\n",
    "        counter = 1\n",
    "\n",
    "        while os.path.exists(path):\n",
    "            path = filename + \" (\" + str(counter) + \")\" + extension\n",
    "            counter += 1\n",
    "\n",
    "        return path\n",
    "    \n",
    "    os.makedirs(os.path.dirname(path_to_save), exist_ok=True)\n",
    "    for i in range(len(X)):\n",
    "        img = np.array(X[i])\n",
    "        label = y[i] \n",
    "        path = (os.path.join(path_to_save, class_names[label], class_names[label]+'.jpeg'))\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        path = uniquify(path)\n",
    "        im = Image.fromarray((img * 255).astype(np.uint8))\n",
    "        im.save(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd1c4343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Saves the data in three separate folders depending if the dataset is for train, val or test\n",
    "\"\"\"\n",
    "@timer\n",
    "def save_data(path, sets, class_names):\n",
    "    paths = ['training', 'validation', 'testing']\n",
    "    for i, p in enumerate(paths):\n",
    "        data = sets[0]\n",
    "        X = sets[i][0]\n",
    "        y = sets[i][1]\n",
    "        p = os.path.join(path, p)\n",
    "        save_images(p, X, y, class_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4499ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def get_dataset(path, class_names, da=0, train=0.7, val=0.1, da_dict={}):     \n",
    "    class_names = [c for c in class_names if c in os.listdir(path)]\n",
    "    full_data = load_dataset(path, class_names)\n",
    "    sets = split_data(full_data, train, val)\n",
    "    train_orig = sets[0]\n",
    "    sets[0] = data_augmentation(da, train_orig[0], train_orig[1], da_dict)\n",
    "    print(class_names)\n",
    "    \n",
    "#     original train size\n",
    "    print(np.bincount(train_orig[1]))\n",
    "#     train size after augmentation\n",
    "    print(np.bincount(sets[0][1]))\n",
    "#     the augmenting multiplication factor\n",
    "    print(np.bincount(sets[0][1]) // np.bincount(train_orig[1]))\n",
    "    \n",
    "    print()\n",
    "    \n",
    "#     the samples for each class for each set\n",
    "    for s in sets:\n",
    "        print(np.bincount(s[1]))\n",
    "        \n",
    "    return (sets, class_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7e69369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_dataset' executed in 11.9943s\n",
      "Function 'split_data' executed in 0.0058s\n",
      "Function 'data_augmentation' executed in 0.0247s\n",
      "['normal', 'ome', 'tube']\n",
      "[113 113  60]\n",
      "[113 113  60]\n",
      "[1 1 1]\n",
      "\n",
      "[113 113  60]\n",
      "[48 48 26]\n",
      "[18 18 10]\n",
      "Function 'get_dataset' executed in 12.0275s\n"
     ]
    }
   ],
   "source": [
    "# Dataset 1\n",
    "res = get_dataset(orig_dataset1, CLASS_NAMES, 0, 0.63, 0.27)\n",
    "\n",
    "# uncomment to save the data\n",
    "# save_data(exp_dataset1, res[0], res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca408cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_dataset' executed in 3.2399s\n",
      "Function 'split_data' executed in 13.8747s\n",
      "Function 'data_augmentation' executed in 6.3304s\n",
      "['normal', 'csom', 'myringosclerosis', 'earwax']\n",
      "[144 144 144 144]\n",
      "[144 144 144 144]\n",
      "[1 1 1 1]\n",
      "\n",
      "[144 144 144 144]\n",
      "[36 36 36 36]\n",
      "[]\n",
      "Function 'get_dataset' executed in 24.2362s\n"
     ]
    }
   ],
   "source": [
    "# Dataset 2\n",
    "res = get_dataset(orig_dataset2, CLASS_NAMES, 0, 0.8, 0.2)\n",
    "\n",
    "# uncomment to save the data\n",
    "# save_data(exp_dataset2, res[0], res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1595733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_dataset' executed in 7.5558s\n",
      "Function 'split_data' executed in 25.9060s\n",
      "Function 'data_augmentation' executed in 72.6842s\n",
      "['aom', 'csom', 'earwax', 'normal']\n",
      "[ 95  50 112 428]\n",
      "[380 400 448 428]\n",
      "[4 8 4 1]\n",
      "\n",
      "[380 400 448 428]\n",
      "[]\n",
      "[ 24  13  28 107]\n",
      "Function 'get_dataset' executed in 107.6194s\n"
     ]
    }
   ],
   "source": [
    "# Dataset 3\n",
    "class_names = ['aom', 'csom', 'earwax', 'normal']\n",
    "\n",
    "res = get_dataset(orig_dataset3, class_names, 350, 0.8, 0)\n",
    "\n",
    "# uncomment to save the data\n",
    "# save_data(exp_dataset3, res[0], res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abfcae2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'load_dataset' executed in 6.9116s\n",
      "Function 'split_data' executed in 0.0036s\n",
      "Function 'data_augmentation' executed in 262.1094s\n",
      "['normal', 'ome', 'tube']\n",
      "[125 125  67]\n",
      "[ 375 1000  536]\n",
      "[3 8 8]\n",
      "\n",
      "[ 375 1000  536]\n",
      "[18 18  9]\n",
      "[36 36 20]\n",
      "Function 'get_dataset' executed in 269.0266s\n"
     ]
    }
   ],
   "source": [
    "# Dataset 4\n",
    "data_aug_1 = {\n",
    "    'normal': 2,\n",
    "    'ome': 7,\n",
    "    'tube': 7  \n",
    "}\n",
    "\n",
    "data_aug_2 = {\n",
    "    'normal': 1,\n",
    "    'csom': 3,\n",
    "    'myringosclerosis': 2,\n",
    "    'earwax': 2\n",
    "}\n",
    "\n",
    "data_aug_3 = {\n",
    "    'normal': 0,\n",
    "    'aom': 11,\n",
    "    'csom': 11,\n",
    "    'myringosclerosis': 23,\n",
    "    'earwax': 5,\n",
    "    'tube': 47  \n",
    "}\n",
    "\n",
    "class_names = CLASS_NAMES\n",
    "\n",
    "# change integer depending on dataset\n",
    "aug = data_aug_1\n",
    "class_names = list(aug.keys())\n",
    "res = get_dataset(orig_dataset1, class_names, 0, 0.7, 0.1, aug)\n",
    "\n",
    "# uncomment to save the data\n",
    "# save_data(exp_dataset4, res[0], res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a35428a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
